{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44860788144562525"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train = pd.read_feather('./X_train_1.feather')\n",
    "y_train = pd.read_feather('./y_train.feather')['prices.amountMax']\n",
    "\n",
    "X_val = pd.read_feather('./X_val_1.feather')\n",
    "y_val = pd.read_feather('./y_val.feather')['prices.amountMax']\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=40, max_features='log2')\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prices.isSale', 'brand_Dansko', 'brand_Ugg', 'brand_other',\n",
       "       'manufacturer_Nike', 'manufacturerNumber_other', 'categories_Athletic',\n",
       "       'categories_Boots', 'categories_Slippers', 'colors_Green'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Evaluate and Draw Conclusions from Random Forest Model\n",
    "#### Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_feather('./X_test.feather')\n",
    "y_test = pd.read_feather('./y_test.feather')['prices.amountMax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42961023864474635"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We get .42 which is .02 lower than the validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "* In this model, I remove a lot of missing values and outliers, and all time-related features.\n",
    "* Since all the features in the final model are boolean, it may affect the prediction score. So not able to get a more accurate number.\n",
    "* From the final features the model chose, isSale was the most significant feature; Some brand names, like Dansko, Ugg Other are selected; Some manufacturer names, like Nike and Other are selected; Some shoe categories, like athletic, boots and slippers are in the final pool; Only one color name, green, remains in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step\n",
    "* My dataset is really restricted on the boolean type data. Cause all of the features are boolean except the date-time features, which in the end I need to remove. I don't have numerical features, so it seems like my model can not achive a higher score, like .7 or .8 . If possible, I would like to import some numerical features next.\n",
    "* But for shoes, the sizes are always in the range from 4 to 12, but with the same price in most situation. And the sale amount is a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Compare against linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy\n",
    "* In my mid-term project with linear regression model with the same dataset, I used 23 features for the final model with a score .20 . And when I use the model to test the testing dataset I only get accuracy of .10, which was really low, half of the model score.\n",
    "* In this project, I used 11 features at last, getting model score .44 . And the accuracy is .42, which performes good.\n",
    "* The reason for the difference is that, I did the model training for only three times, and in each time, I removed too many features by the correlation analysis guidance each step. But in the random forest regression model, every time it just removes the features under .005 weight or .01 . It's much more logical doing in this processment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances\n",
    "* The linear regreesion model's important features, 23 features:\n",
    "       'brand_Asics', 'brand_Charles by Charles David', 'brand_Converse',\n",
    "       'brand_Crocs', 'brand_MICHAEL Michael Kors', 'brand_Propet',\n",
    "       'brand_Telic', 'brand_Ugg', 'brand_Under Armour',\n",
    "       'manufacturer_Charles by Charles David', 'manufacturer_Converse',\n",
    "       'manufacturer_Crocs', 'manufacturer_MICHAEL Michael Kors',\n",
    "       'manufacturer_Novascarpa Group LLC', 'manufacturer_Propet',\n",
    "       'manufacturer_Under Armour', 'manufacturer_asics',\n",
    "       'manufacturerNumber_5825', 'manufacturerNumber_Anna',\n",
    "       'prices.merchant_Overstock.com', 'prices.merchant_Shoes.com',\n",
    "       'prices.merchant_Walmart', 'prices.merchant_other'\n",
    "* The random forest model's important features, 11 features:\n",
    "       'prices.isSale', 'brand_Dansko', 'brand_Ugg', 'brand_other',\n",
    "       'manufacturer_Nike', 'manufacturerNumber_other', 'categories_Athletic',\n",
    "       'categories_Boots', 'categories_Slippers', 'colors_Green'\n",
    "\n",
    "* Most of them are different, since the linear regression model did a bad accuracy. I don't trust its result on importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "* In my case, which includes too many categorical features, it's much more difficult for linear regreesion model to draw a fitting model and make a good reasonable prediction. And we only use the correlation analysis method to remove features, which is more subjective to decide where to draw the line.\n",
    "* It's much more suitable to apply random forest regression model on my case, cause it's all about boolean, which goes left or right. Using permutation importances way enables you to remove a few features step by step, which is more convenient and effictive. What's more important, it's more logical to remove the features with a under .01 weight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
